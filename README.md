
# LLMBenchGPT ‚Äî Lightweight Benchmarking of Free LLMs via OpenRouter

This project is a lightweight framework for evaluating open-access large language models (LLMs) using the [OpenRouter.ai](https://openrouter.ai) API. It focuses on the real-world generation quality of dozens of models using a consistent prompt format and tone control.

## Purpose

LLMBenchGPT was created to explore:
- factual accuracy and hallucination rate
- stylistic diversity between models
- responsiveness and latency differences
- compatibility with long prompts and instructions

This project **does not train models**, but instead evaluates how existing free models perform when given the same real-world content creation task.

## Contents

- `ai_blogger_batch.py`: main script that loops through 50+ models and prompts them in the same way
- `examples/`: structured folder with model-specific generations (e.g. `examples/meta-llama/llama-3.2-1b-instruct_post.md`)
- `config.py`: optional API setup or helper configuration

## Evaluation Task

Each model is asked to generate a **blog-style post** on the same topic in the same tone.

### Example prompt:
> Write a detailed blog-style post about the topic 'Top 5 AI models 2025' in a 'concise and factual' tone.  
> Do not make anything up ‚Äî mention only real facts or real companies if applicable.  
> Make the writing catchy and engaging.

### Topics used:
- Top 5 LLM models 2025
- Best AI tools for developers
- What's new in open-source AI

## Results (Preview)

| Model                                           | Tone                        | Structure                        | Factuality          | Hallucinations | Length |
|-------------------------------------------------|-----------------------------|----------------------------------|---------------------|----------------|--------|
| bytedance-research/ui-tars-72b                  | Conversational + Marketing  | Bulleted + Markdown              | ‚ö†Ô∏è Mostly accurate  | Minor          | Medium |
| cognitivecomputations/dolphin3.0-mistral-24b    | Enthusiastic + Expert-style | Well-structured Markdown         | ‚ö†Ô∏è Mostly accurate  | Moderate       | Long   |
| cognitivecomputations/dolphin3.0-r1-mistral-24b | Playful + Experimental	     | Bulleted + Markdown	             | ‚ùå Inaccurate	       | üî• Heavy	      | Medium |
| deepseek/deepseek-chat-v3-0324                  | Neutral + Informative		     | Well-structured Markdown		       | ‚úÖ Accurate		        | None	          | Medium |
| deepseek/deepseek-r1                            | Professional + Inspiring			 | Well-structured Markdown	        | ‚úÖ Accurate	         | None	          | Medium |
| deepseek/deepseek-r1-distill-llama-70b          | Neutral + Marketing-ish	    | Well-structured Markdown	        | ‚ö†Ô∏è Mostly accurate	 | Moderate	      | Long   |
| deepseek/deepseek-r1-distill-qwen-14b           | Visionary + Promotional	    | Well-structured Markdown	        | ‚ùå Inaccurate	       | Heavy	         | Long   |
| deepseek/deepseek-r1-distill-qwen-32b           | Promotional + Accessible		  | Bulleted Markdown	               | ‚ùå Inaccurate	       | Heavy	         | Medium |
| deepseek/deepseek-r1-zero                       | Educational + Technical	    | Full Markdown Doc	               | ‚úÖ Accurate	         | None	          | Long   |
| deepseek-v3-base_post                           | ‚úñ Corrupted Output	         | ‚úñ N/A	                           | ‚úñ N/A	              | ‚úñ N/A	         | ‚úñ N/A	 |
| featherless/qwerky-72b                          | Promotional + Visionary     | Well-structured Markdown         | ‚ùå Inaccurate	       | üî• Heavy       | Long   |
| google/gemini-2.0-flash-exp                     | Bold + Conversational	      | Headings + Commentary	           | ‚úÖ Mostly true	      | ‚ö†Ô∏è Light	      | Long   |
| google/gemini-2.0-flash-thinking-exp-1219       | Bold + Informal + Engaging  | Bulleted highlights + sections 	 | ‚úÖ Mostly true	      | ‚ö†Ô∏è Light	      | Long   |
| google/gemini-flash-1.5-8b-exp                  | Speculative + Futuristic    | Numbered list + summary          | ‚ö†Ô∏è Partial          | ‚ö†Ô∏è Medium      | Medium |
| google/gemma-2-9b-it                            | Fun + Pop-style             | Numbered list + hype             | ‚úÖ Mostly true       | ‚ö†Ô∏è Light       | Medium |
| google/gemma-3-1b-it                            | Informative + Practical     | Numbered list + summaries        | ‚úÖ Mostly true       | ‚ö†Ô∏è Light       | Long   |
| google/gemma-3-4b-it                            | Catchy + Professional       | Numbered list + key points       | ‚úÖ True              | ‚ö†Ô∏è Light       | Medium |
| google/gemma-3-12b-it                           | Fun + Informative           | Numbered list with notes         | ‚úÖ True              | ‚ö†Ô∏è None        | Medium |
| google/gemma-3-27b-it                           | Concise + Friendly          | Numbered + Block Format          | ‚úÖ True              | ‚ùå None         | Medium |
| google/learnlm-1.5-pro-experimental             | Brief + Direct              | Numbered bullet list             | ‚úÖ True              | ‚ùå None         | Short  |
| meta-llama/llama-3.1-8b-instruct                | Promotional                 | Numbered list + intro/outro      | ‚ö† Partial           | üî• Heavy       | Medium |
| meta-llama/llama-3.2-1b-instruct                | Promotional + Inflated      | Numbered list + intro/outro      | ‚ùå Inaccurate        | üî• Heavy       | Long   |
| meta-llama/llama-3.2-3b-instruct                | Promotional + Optimistic    | Numbered list + intro/outro      | ‚ùå Inaccurate        | üî• Heavy       | Medium |
| meta-llama/llama-3.2-11b-vision-instruct        | Playful + Informative       | Numbered list + intro/outro      | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| meta-llama/llama-3.3-70b-instruct               | Upbeat + Informative        | Numbered list + intro/outro      | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| mistralai/mistral-7b-instruct                   | Optimistic + Creative       | Numbered list + inspirational    | ‚ùå Inaccurate        | üî• Heavy       | Medium |
| mistralai/mistral-nemo                          | Narrative + Informative     | Numbered list + highlights       | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| mistralai/mistral-small-24b-instruct-2501       | Friendly + Clear            | Numbered + Bonus + Summary       | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| moonshotai/kimi-vl-a3b-thinking                 | Bold + Satirical            | Numbered list + punchlines       | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| nousresearch/deephermes-3-llama-3-8b-preview    | Upbeat + Descriptive        | Numbered list + soft outro       | ‚ùå Inaccurate        | üî• Heavy       | Medium |
| llama-3.1-nemotron-70b-instruct                 | Energetic + Professional    | Numbered list + blurbs           | ‚úÖ Mostly true       | ‚ö†Ô∏è Light       | Medium |
| llama-3.1-nemotron-nano-8b-v1                   | Neutral + Analytical        | Numbered list + rationale        | ‚ùå Inaccurate        | üî• Heavy       | Medium |
| llama-3.1-nemotron-ultra-253b-v1                | Friendly + Promotional      | Numbered list + call to action   | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| llama-3.3-nemotron-super-49b-v1                 | Professional + Uplifting    | Numbered + Highlights + Outro    | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| open-r1/olympiccoder-32b                        | Professional + Balanced     | Numbered list + brief summary    | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| openrouter/optimus-alpha                        | Confident + Practical       | Numbered list + pros/highlights  | ‚úÖ Accurate          | ‚ùå None         | Medium |
| qwen/qwen-2.5-7b-instruct                       | Neutral + Informative       | Numbered list + conclusion       | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| qwen/qwen-2.5-72b-instruct                      | Informative + Promotional   | Headings + Short paragraphs      | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| qwen/qwen-2.5-coder-32b-instruct                | Neutral + Descriptive       | Numbered list + summary block    | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| qwen/qwen2.5-vl-3b-instruct                     | Neutral + Repetitive        | Numbered list + blurbs           | ‚ùå Inaccurate        | üî• Heavy       | Medium |
| qwen/qwen2.5-vl-32b-instruct                    | Professional + Visionary    | Headings + Paragraphs            | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Long   |
| qwen/qwen2.5-vl-72b-instruct                    | Friendly + Promotional      | Numbered list + outro            | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| qwen/qwq-32b                                    | Confident + Clear           | Headings + bullet summaries      | ‚úÖ Accurate          | ‚ùå None         | Medium |
| rekaai/reka-flash-3                             | Catchy + Insightful         | Headings + "Why 2025" sections   | ‚ö†Ô∏è Mostly accurate  | ‚ö†Ô∏è Light       | Medium |
| rogue-rose-103b-v0.2                            | Inspiring + Informative     | Numbered list + intro/concl.     | ‚úÖ Accurate          | ‚ùå None         | Long   |

## How to Run

1. Install dependencies:
```bash
pip install requests
```
1. Run the batch script:
```bash
python ai_blogger_batch.py
```
1. Select your topic + style. Results are saved to:
```
/examples/<model_id>_post.md
```

## Why this matters

Most developers rely on a small handful of LLMs (usually OpenAI or Claude). This project demonstrates:
- How to integrate 50+ models using the same backend (OpenRouter)
- How to compare models fairly using prompt-engineering constraints
- How drastically outputs vary even for "similar" models
- How to store and inspect outputs systematically

## License

MIT ‚Äî free to use, fork, and modify. Attribution appreciated.
